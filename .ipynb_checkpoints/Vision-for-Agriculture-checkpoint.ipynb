{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contemporary-sound",
   "metadata": {},
   "source": [
    "\n",
    "## Vision-for-Agriculture\n",
    "Vision for Agriculture Segmenting and classifying aerial images of US farmland\n",
    "Dataset intro\n",
    "The challenge dataset contains 21,061 aerial farmland images captured throughout 2019 across the US. Each image consists of four 512x512 color channels, which are RGB and Near Infra-red (NIR). Each image also has a boundary map and a mask. The boundary map indicates the region of the farmland, and the mask indicates valid pixels in the image. Regions outside of either the boundary map or the mask are not evaluated.\n",
    "\n",
    "This dataset contains six types of annotations: Cloud shadow, Double plant, Planter skip, Standing Water, Waterway, and Weed cluster. These types of field anomalies have great impacts on the potential yield of farmlands, therefore it is extremely important to accurately locate them. In the Agriculture-Vision dataset, these six patterns are stored separately as binary masks due to potential overlaps between patterns. Users are free to decide how to use these annotations.\n",
    "\n",
    "Each field image has a file name in the format of (field id)_(x1)-(y1)-(x2)-(y2).(jpg/png). Each field id uniquely identifies the farmland that the image is cropped from, and (x1, y1, x2, y2) is a 4-tuple indicating the position in which the image is cropped. Please refer to our paper for more details regarding how we construct the dataset.\n",
    "\n",
    "Your task\n",
    "Using the dataset described above your task is to train a model to predict field anomalies on new images. Given a new input from the test set your task is to predict what class does each pixel belong to (one of the six anomalies or the background).\n",
    "\n",
    "Submission\n",
    "This year your submissions will not go through the Kaggle website. Due to issues with the privacy of the test set, you will use a platform codalab: https://competitions.codalab.org/competitions/23732\n",
    "\n",
    "It is straightforward to use - you need to register your team and you can upload your predictions (up to 2 per day and 999 in total). You will see your scores instantly. Make sure your team name on codalab is in the following format \"comp540_netid_netid_*\". This will be important for us to be able to see your results, keep track of your progress and extract the class leaderboard. Also beware - the scoring process on codalab takes about 6h!\n",
    "\n",
    "The server expects you to format the predictions in the following way:\n",
    "\n",
    "- for each image in the test set you should produce a prediction image in .png format, with filename **field-id_x1-y1-x2-y2.png**. The image ID 'field-id_x1-y1-x2-y2' must match the ID of the predicted image exactly. Each png file in your submission will be loaded using\n",
    "\n",
    "numpy.array(PIL.Image.open(‘field-id_x1-y1-x2-y2.png’))\n",
    "\n",
    "So we recommend you save the predictions using\n",
    "\n",
    "PIL.Image.fromarray(pred).save(‘field-id_x1-y1-x2-y2.png’)\n",
    "\n",
    "- the prediction image should have the same size as the input image and each pixel should contain a label from 0-6 indicating the anomaly type/background. Specifically:\n",
    "\n",
    "0 - background\n",
    "\n",
    "1 - cloud_shadow\n",
    "\n",
    "2 - double_plant\n",
    "\n",
    "3 - planter_skip\n",
    "\n",
    "4 - standing_water\n",
    "\n",
    "5 - waterway\n",
    "\n",
    "6 - weed_cluster\n",
    "\n",
    "This label order will be strictly followed during evaluation.\n",
    "\n",
    "- then you should zip the folder with prediction images and upload it to codalab.\n",
    "\n",
    "Evaluation metric\n",
    "We use mean Intersection-over-Union (mIoU) as our main quantitative evaluation metric, which is one of the most commonly used measures in semantic segmentation datasets. The mIoU is computed as:\n",
    "\n",
    "\n",
    "\n",
    "Where c is the number of annotation types (c = 7 in our dataset, with 6 patterns + background), Pc and Tc are the predicted mask and ground truth mask of class c respectively.\n",
    "\n",
    "Since our annotations may overlap, we modify the canonical mIoU metric to accommodate this property. For pixels with multiple labels, a prediction of either label will be counted as a correct pixel classification for that label, and a prediction that does not contain any ground truth labels will be counted as an incorrect classification for all ground truth labels.\n",
    "\n",
    "A more detailed explanation can be found here: https://github.com/SHI-Labs/Agriculture-Vision .\n",
    "\n",
    "First steps\n",
    "We want to encourage you to take the following steps first:\n",
    "\n",
    "1) Check out the \"Data\" tab, download the data and get familiar with the available dataset. More details about the dataset are under the Data tab.\n",
    "\n",
    "2) Check out the following page where the original competition instructions are placed: https://github.com/SHI-Labs/Agriculture-Vision\n",
    "\n",
    "3) Make an account on codalab and register your team here: https://competitions.codalab.org/competitions/23732\n",
    "\n",
    "4) Start experimenting with simple models and get your submissions up on the server\n",
    "\n",
    "IMPORTANT: make sure your team name on codalab is in the following format \"comp540_netid_netid_*\". This will be important for us to be able to see your results and extract the class leaderboard!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "iraqi-problem",
   "metadata": {},
   "source": [
    "## Apply mask and boundry to the original picture for both RBG and NIR channel\n",
    "Build parsers to help loading data\n",
    "Helper functions are included in utils.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sporting-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import utils\n",
    "from matplotlib import image as mpimg\n",
    "import matplotlib\n",
    "import random\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10, 8.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "signed-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agri_version(train_dir,labels_dir):\n",
    "    '''\n",
    "    Load label data according to the required 6 classes\n",
    "    Input: Train_directory ---- str\n",
    "           Labels_directory ---- str\n",
    "    Output: Label data ---- dict[list]   \n",
    "    '''\n",
    "    labels_init = defaultdict(list)\n",
    "    # initialize label dictionary according to the training data\n",
    "    for f in [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]:\n",
    "        labels_init[f[0:-4]] = [0,0,0,0,0,0]\n",
    "    for ind, l in enumerate([f for f in os.listdir(labels_dir)][1:]):\n",
    "        direc = labels_dir + l\n",
    "        for file in tqdm([f for f in os.listdir(direc) if os.path.isfile(os.path.join(direc, f))]):\n",
    "            if file != \".DS_Store\":\n",
    "                # Calculate coverage according to the corresponding label\n",
    "                rate = (np.array(Image.open(os.path.join(direc, file)).convert('L'))/255).mean()\n",
    "                labels_init[file[0:-4]][ind] = rate\n",
    "    # save dictionary for later use\n",
    "    f = open(\"y_train.pkl\",\"wb\")\n",
    "    pickle.dump(labels_init,f)\n",
    "    f.close()\n",
    "    return labels_init\n",
    "\n",
    "train_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/images/rgb\"\n",
    "labels_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/labels/\"\n",
    "y_train = utils.load_agri_version(train_dir,labels_dir,\"y_train\")\n",
    "\n",
    "\n",
    "val_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/images/rgb\"\n",
    "val_labels_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/labels/\"\n",
    "y_val = utils.load_agri_version(val_dir,val_labels_dir,\"y_val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-microwave",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_agri_version(test_dir):\n",
    "    '''\n",
    "    Input: Test_directory ---- str\n",
    "    Output: Label data ---- dict[list]   \n",
    "    '''\n",
    "    labels_init = defaultdict(list)\n",
    "    # initialize label dictionary according to the training data\n",
    "    for f in [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]:\n",
    "        labels_init[f[0:-4]] = None\n",
    "    # save dictionary for later use\n",
    "    f = open(\"y_test.pkl\",\"wb\")\n",
    "    pickle.dump(labels_init,f)\n",
    "    f.close()\n",
    "    return labels_init\n",
    "test_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/test/images/rgb\"\n",
    "load_argi_version(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "enclosed-playlist",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('y_train.pkl', 'rb')     \n",
    "db = pickle.load(dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dried-timing",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfile = open('y_val.pkl','rb')\n",
    "db = pickle.load(dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-mainstream",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_argment_train(fig_id):\n",
    "    '''\n",
    "    Process single image in validation set\n",
    "    Input: Image Id ---- str\n",
    "    Output: Processed image (Apply mask + boundry, stack NIR) ---- np.array\n",
    "    '''\n",
    "    rgb_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/images/rgb'\n",
    "    nir_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/images/nir'\n",
    "    bdry_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/boundaries'\n",
    "    mask_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/masks'\n",
    "    \n",
    "    rgb_img = mpimg.imread(os.path.join(rgb_path, fig_id + '.jpg'))/255\n",
    "    nir_img = mpimg.imread(os.path.join(nir_path, fig_id + '.jpg')).reshape((512,512,1))/255\n",
    "    bdry_img = mpimg.imread(os.path.join(bdry_path, fig_id + '.png'))\n",
    "    mask_img = mpimg.imread(os.path.join(mask_path, fig_id + '.png'))\n",
    "    #input_img = np.concatenate([rgb_img, nir_img], axis=2) / 255. # Concatenate the RGB and NIR\n",
    "    \n",
    "    # stack mask and boundry\n",
    "    final_mask = np.multiply(mask_img,bdry_img).reshape(512,512,1)\n",
    "    \n",
    "    # add nir channel to original data\n",
    "    input_img = np.concatenate([rgb_img, nir_img], axis=2) / 255. # Concatenate the RGB and NIR\n",
    "    \n",
    "    # apply mask and boundry to image\n",
    "    final_img_rgb = np.multiply(final_mask,rgb_img)\n",
    "    final_img_nir = np.multiply(final_mask,nir_img)\n",
    "\n",
    "    return final_img_rgb, final_img_nir\n",
    "\n",
    "final_img_rgb, final_img_nir= utils.image_argment_train('UI36U6X7A_984-5109-1496-5621')   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "congressional-premium",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_argment_val(fig_id):\n",
    "    '''\n",
    "    Process single image in validation set\n",
    "    Input: Image Id ---- str\n",
    "    Output: Processed image (Apply mask + boundry, stack NIR) ---- np.array\n",
    "    '''\n",
    "    rgb_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/images/rgb'\n",
    "    nir_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/images/nir'\n",
    "    bdry_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/boundaries'\n",
    "    mask_path = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/masks'\n",
    "    \n",
    "    rgb_img = mpimg.imread(os.path.join(rgb_path, fig_id + '.jpg'))/255\n",
    "    nir_img = mpimg.imread(os.path.join(nir_path, fig_id + '.jpg')).reshape((512,512,1))/255\n",
    "    bdry_img = mpimg.imread(os.path.join(bdry_path, fig_id + '.png'))\n",
    "    mask_img = mpimg.imread(os.path.join(mask_path, fig_id + '.png'))\n",
    "    #input_img = np.concatenate([rgb_img, nir_img], axis=2) / 255. # Concatenate the RGB and NIR\n",
    "    \n",
    "    # stack mask and boundry\n",
    "    final_mask = np.multiply(mask_img,bdry_img).reshape(512,512,1)\n",
    "    \n",
    "    # add nir channel to original data\n",
    "    input_img = np.concatenate([rgb_img, nir_img], axis=2) / 255. # Concatenate the RGB and NIR\n",
    "    \n",
    "    # apply mask and boundry to image\n",
    "    final_img_rgb = np.multiply(final_mask,rgb_img)\n",
    "    final_img_nir = np.multiply(final_mask,nir_img)\n",
    "\n",
    "    return final_img_rgb, final_img_nir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "casual-bankruptcy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_process_img_train (fig_id, path_tar_rgb, path_tar_nir, resize_x = 512, resize_y = 512):\n",
    "    \n",
    "    '''\n",
    "    Rescale the processed image and save to the required directory\n",
    "    Input: fig_id ---- str\n",
    "           path_tar_rgb ---- str\n",
    "           path_tar_nir ---- nir\n",
    "           resize_x ---- int\n",
    "           resize_y ---- int\n",
    "    '''\n",
    "    try:\n",
    "        path_tar_rgb = os.path.join(path_tar_rgb,str(resize_x) + \"*\" + str(resize_y))\n",
    "        os.makedirs(path_tar_rgb)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    try:\n",
    "        path_tar_nir = os.path.join(path_tar_nir,str(resize_x) + \"*\" + str(resize_y))\n",
    "        os.makedirs(path_tar_nir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # process rgb and nir data with mask and boundry\n",
    "    final_img_rgb, final_img_nir = image_argment_train(fig_id)\n",
    "    # resize and save rgb data\n",
    "    matplotlib.image.imsave(os.path.join(path_tar_rgb, fig_id + '.jpg'), final_img_rgb)\n",
    "    rgb_resize = Image.open(os.path.join(path_tar_rgb, fig_id + '.jpg')).resize((resize_x,resize_y))\n",
    "    rgb_resize.save(os.path.join(path_tar_rgb, fig_id + '.jpg'),quality = 90)\n",
    "    \n",
    "    # resize and save nir data\n",
    "    final_img_nir = np.concatenate([final_img_nir, final_img_nir, final_img_nir], axis=2)\n",
    "    matplotlib.image.imsave(os.path.join(path_tar_nir, fig_id + '.jpg'), final_img_nir)\n",
    "    nir_resize = Image.open(os.path.join(path_tar_nir, fig_id + '.jpg')).resize((resize_x,resize_y))\n",
    "    nir_resize.save(os.path.join(path_tar_nir, fig_id + '.jpg'),quality = 90)\n",
    "    \n",
    "    return rgb_resize, nir_resize\n",
    "\n",
    "path_tar_rgb = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/rgb'\n",
    "path_tar_nir = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/nir'\n",
    "rgb, nir = utils.save_process_img_train ('UI36U6X7A_984-5109-1496-5621', path_tar_rgb, path_tar_nir,256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "occupied-business",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_process_img_val (fig_id, path_tar_rgb, path_tar_nir, resize_x = 512, resize_y = 512):\n",
    "    \n",
    "    '''\n",
    "    Rescale the processed image and save to the required directory\n",
    "    Input: fig_id ---- str\n",
    "           path_tar_rgb ---- str\n",
    "           path_tar_nir ---- nir\n",
    "           resize_x ---- int\n",
    "           resize_y ---- int\n",
    "    '''\n",
    "    try:\n",
    "        path_tar_rgb = os.path.join(path_tar_rgb,str(resize_x) + \"*\" + str(resize_y))\n",
    "        os.makedirs(path_tar_rgb)\n",
    "    except:\n",
    "        pass\n",
    "            \n",
    "    try:\n",
    "        path_tar_nir = os.path.join(path_tar_nir,str(resize_x) + \"*\" + str(resize_y))\n",
    "        os.makedirs(path_tar_nir)\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    # process rgb and nir data with mask and boundry\n",
    "    final_img_rgb, final_img_nir = image_argment_val(fig_id)\n",
    "    # resize and save rgb data\n",
    "    matplotlib.image.imsave(os.path.join(path_tar_rgb, fig_id + '.jpg'), final_img_rgb)\n",
    "    rgb_resize = Image.open(os.path.join(path_tar_rgb, fig_id + '.jpg')).resize((resize_x,resize_y))\n",
    "    rgb_resize.save(os.path.join(path_tar_rgb, fig_id + '.jpg'),quality = 90)\n",
    "    \n",
    "    # resize and save nir data\n",
    "    final_img_nir = np.concatenate([final_img_nir, final_img_nir, final_img_nir], axis=2)\n",
    "    matplotlib.image.imsave(os.path.join(path_tar_nir, fig_id + '.jpg'), final_img_nir)\n",
    "    nir_resize = Image.open(os.path.join(path_tar_nir, fig_id + '.jpg')).resize((resize_x,resize_y))\n",
    "    nir_resize.save(os.path.join(path_tar_nir, fig_id + '.jpg'),quality = 90)\n",
    "    \n",
    "    return rgb_resize, nir_resize\n",
    "\n",
    "path_tar_rgb = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/val/rgb'\n",
    "path_tar_nir = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/val/nir'\n",
    "rgb, nir = utils.save_process_img_val ('1J7JL2PDK_2303-3001-2815-3513', path_tar_rgb, path_tar_nir,256,256)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "settled-behalf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_train(train_data, path_tar_rgb, path_tar_nir, resize_x = 30, resize_y = 30):\n",
    "    '''\n",
    "    Process training data, resize and save images of RGB and NIR channel\n",
    "    Input: train_data ---- database.pkl\n",
    "           path_tar_rgb ---- str\n",
    "           path_tar_nir ---- str\n",
    "           resize_x ---- int\n",
    "           resize_y ---- int\n",
    "    Output: None \n",
    "    '''\n",
    "    dbfile = open(train_data, 'rb')     \n",
    "    db = pickle.load(dbfile)\n",
    "    for fig_id in tqdm(db):\n",
    "        save_process_img_train (fig_id, path_tar_rgb, path_tar_nir, resize_x, resize_y)\n",
    "    return None \n",
    "            \n",
    "train_data = 'y_train.pkl'\n",
    "path_tar_rgb = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/rgb'\n",
    "path_tar_nir = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/nir'\n",
    "process_tarin(train_data, path_tar_rgb, path_tar_nir, resize_x = 30, resize_y = 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fatal-prize",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4431/4431 [05:31<00:00, 13.35it/s]\n"
     ]
    }
   ],
   "source": [
    "def process_val(train_data, path_tar_rgb, path_tar_nir, resize_x = 30, resize_y = 30):\n",
    "    '''\n",
    "    Process training data, resize and save images of RGB and NIR channel\n",
    "    Input: train_data ---- database.pkl\n",
    "           path_tar_rgb ---- str\n",
    "           path_tar_nir ---- str\n",
    "           resize_x ---- int\n",
    "           resize_y ---- int\n",
    "    Output: None \n",
    "    '''\n",
    "    dbfile = open(train_data, 'rb')     \n",
    "    db = pickle.load(dbfile)\n",
    "    for fig_id in tqdm(db):\n",
    "        utils.save_process_img_val (fig_id, path_tar_rgb, path_tar_nir, resize_x, resize_y)\n",
    "    return None \n",
    "val_data = 'y_val.pkl'\n",
    "path_tar_rgb = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/val/rgb'\n",
    "path_tar_nir = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/val/nir'\n",
    "process_val(val_data, path_tar_rgb, path_tar_nir, resize_x = 512, resize_y = 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "desperate-toronto",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-latino",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_tar_rgb = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/rgb'\n",
    "path_tar_nir = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/nir'\n",
    "test = utils.save_process_img_train ('UI36U6X7A_984-5109-1496-5621', path_tar_rgb, path_tar_nir,64,64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broke-composition",
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsample_tarin_flatened(train_data, number_train):\n",
    "    '''\n",
    "    Subsample the training set to number_training set\n",
    "    Input:\n",
    "    Output:\n",
    "    '''\n",
    "    dbfile = open(train_data, 'rb')     \n",
    "    db = pickle.load(dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "developing-hours",
   "metadata": {},
   "outputs": [],
   "source": [
    "count, tar_num = 0, 12901\n",
    "dict_sub = {}\n",
    "dbfile = open('y_train.pkl', 'rb')     \n",
    "db = pickle.load(dbfile)\n",
    "path_tar_rgb = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/rgb'\n",
    "path_tar_nir = '/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/Process/train/nir'\n",
    "while count < tar_num:\n",
    "    key = random.choice(list(db.keys()))\n",
    "    if key not in dict_sub:\n",
    "        dict_sub[key] = db[key]\n",
    "        count += 1\n",
    "for key in tqdm(dict_sub):\n",
    "    rgb, nir = utils.save_process_img_train (key,path_tar_rgb, path_tar_nir,512,512)\n",
    "    #flat_img = list(np.array(rgb).flatten())\n",
    "    #dict_sub[key] += flat_img\n",
    "    \n",
    "#df = pd.DataFrame(dict_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "residential-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('flatened_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-aquarium",
   "metadata": {},
   "source": [
    "### Provide statistic result for y_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loaded-purchase",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'y_train.pkl'\n",
    "dbfile = open(train_data, 'rb')     \n",
    "db = pickle.load(dbfile)\n",
    "y_train = np.array(pd.DataFrame(db)).T\n",
    "bg = (1 - y_train.sum(axis = 1) ).reshape(-1,1)\n",
    "y_train_wb = np.hstack((bg,y_train)).argmax(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-consumer",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = sorted(Counter(y_train_wb).items(), key = lambda x: x[0])\n",
    "label = [x[1]/12901 for x in labels_train]\n",
    "classes = ['background','cloud_shadow','double_plant','planter_skip','standing_water','waterway','weed_cluster']\n",
    "x_pos = np.arange(len(label))\n",
    "# Create bars with different colors\n",
    "plt.bar(x_pos, label, color = sns.color_palette(\"Set2\")[:7])\n",
    "# Create names on the x-axis\n",
    "plt.xticks(x_pos, classes)\n",
    "# Show graph\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-pregnancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack((bg,y_train)))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "level-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns = ['Background', 'Cloud', 'Double Plant', 'Planter Skip', 'Standing Water', 'Waterway', 'Weed Cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-translation",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(np.hstack((bg,y_train)))\n",
    "df.columns = ['Background', 'Cloud', 'Double Plant', 'Planter Skip', 'Standing Water', 'Waterway', 'Weed Cluster']\n",
    "df.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-costs",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upper-liabilities",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-owner",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aboriginal-hughes",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'y_train.pkl'\n",
    "dbfile = open(train_data, 'rb')  \n",
    "db = pickle.load(dbfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "essential-jesus",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(db)\n",
    "df.to_csv('y_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-robinson",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = 'y_val.pkl'\n",
    "dbfile = open(train_data, 'rb')  \n",
    "db = pickle.load(dbfile)\n",
    "df = pd.DataFrame(db)\n",
    "df.to_csv('y_val.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-biodiversity",
   "metadata": {},
   "source": [
    "### DNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from glob import glob\n",
    "import IPython.display as display\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import time\n",
    "from tensorflow.keras.layers import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "try:\n",
    "    os.makedirs('Image_process')\n",
    "except OSError as e:\n",
    "    if e.errno != errno.EEXIST:\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
