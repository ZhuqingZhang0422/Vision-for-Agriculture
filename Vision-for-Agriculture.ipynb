{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "contemporary-sound",
   "metadata": {},
   "source": [
    "\n",
    "## Vision-for-Agriculture\n",
    "Vision for Agriculture Segmenting and classifying aerial images of US farmland\n",
    "Dataset intro\n",
    "The challenge dataset contains 21,061 aerial farmland images captured throughout 2019 across the US. Each image consists of four 512x512 color channels, which are RGB and Near Infra-red (NIR). Each image also has a boundary map and a mask. The boundary map indicates the region of the farmland, and the mask indicates valid pixels in the image. Regions outside of either the boundary map or the mask are not evaluated.\n",
    "\n",
    "This dataset contains six types of annotations: Cloud shadow, Double plant, Planter skip, Standing Water, Waterway, and Weed cluster. These types of field anomalies have great impacts on the potential yield of farmlands, therefore it is extremely important to accurately locate them. In the Agriculture-Vision dataset, these six patterns are stored separately as binary masks due to potential overlaps between patterns. Users are free to decide how to use these annotations.\n",
    "\n",
    "Each field image has a file name in the format of (field id)_(x1)-(y1)-(x2)-(y2).(jpg/png). Each field id uniquely identifies the farmland that the image is cropped from, and (x1, y1, x2, y2) is a 4-tuple indicating the position in which the image is cropped. Please refer to our paper for more details regarding how we construct the dataset.\n",
    "\n",
    "Your task\n",
    "Using the dataset described above your task is to train a model to predict field anomalies on new images. Given a new input from the test set your task is to predict what class does each pixel belong to (one of the six anomalies or the background).\n",
    "\n",
    "Submission\n",
    "This year your submissions will not go through the Kaggle website. Due to issues with the privacy of the test set, you will use a platform codalab: https://competitions.codalab.org/competitions/23732\n",
    "\n",
    "It is straightforward to use - you need to register your team and you can upload your predictions (up to 2 per day and 999 in total). You will see your scores instantly. Make sure your team name on codalab is in the following format \"comp540_netid_netid_*\". This will be important for us to be able to see your results, keep track of your progress and extract the class leaderboard. Also beware - the scoring process on codalab takes about 6h!\n",
    "\n",
    "The server expects you to format the predictions in the following way:\n",
    "\n",
    "- for each image in the test set you should produce a prediction image in .png format, with filename **field-id_x1-y1-x2-y2.png**. The image ID 'field-id_x1-y1-x2-y2' must match the ID of the predicted image exactly. Each png file in your submission will be loaded using\n",
    "\n",
    "numpy.array(PIL.Image.open(‘field-id_x1-y1-x2-y2.png’))\n",
    "\n",
    "So we recommend you save the predictions using\n",
    "\n",
    "PIL.Image.fromarray(pred).save(‘field-id_x1-y1-x2-y2.png’)\n",
    "\n",
    "- the prediction image should have the same size as the input image and each pixel should contain a label from 0-6 indicating the anomaly type/background. Specifically:\n",
    "\n",
    "0 - background\n",
    "\n",
    "1 - cloud_shadow\n",
    "\n",
    "2 - double_plant\n",
    "\n",
    "3 - planter_skip\n",
    "\n",
    "4 - standing_water\n",
    "\n",
    "5 - waterway\n",
    "\n",
    "6 - weed_cluster\n",
    "\n",
    "This label order will be strictly followed during evaluation.\n",
    "\n",
    "- then you should zip the folder with prediction images and upload it to codalab.\n",
    "\n",
    "Evaluation metric\n",
    "We use mean Intersection-over-Union (mIoU) as our main quantitative evaluation metric, which is one of the most commonly used measures in semantic segmentation datasets. The mIoU is computed as:\n",
    "\n",
    "\n",
    "\n",
    "Where c is the number of annotation types (c = 7 in our dataset, with 6 patterns + background), Pc and Tc are the predicted mask and ground truth mask of class c respectively.\n",
    "\n",
    "Since our annotations may overlap, we modify the canonical mIoU metric to accommodate this property. For pixels with multiple labels, a prediction of either label will be counted as a correct pixel classification for that label, and a prediction that does not contain any ground truth labels will be counted as an incorrect classification for all ground truth labels.\n",
    "\n",
    "A more detailed explanation can be found here: https://github.com/SHI-Labs/Agriculture-Vision .\n",
    "\n",
    "First steps\n",
    "We want to encourage you to take the following steps first:\n",
    "\n",
    "1) Check out the \"Data\" tab, download the data and get familiar with the available dataset. More details about the dataset are under the Data tab.\n",
    "\n",
    "2) Check out the following page where the original competition instructions are placed: https://github.com/SHI-Labs/Agriculture-Vision\n",
    "\n",
    "3) Make an account on codalab and register your team here: https://competitions.codalab.org/competitions/23732\n",
    "\n",
    "4) Start experimenting with simple models and get your submissions up on the server\n",
    "\n",
    "IMPORTANT: make sure your team name on codalab is in the following format \"comp540_netid_netid_*\". This will be important for us to be able to see your results and extract the class leaderboard!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sporting-advice",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from PIL import Image\n",
    "from numpy import asarray\n",
    "from matplotlib import image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "formal-money",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "f = open(\"train_label.pkl\",\"wb\")\n",
    "pickle.dump(dict,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "middle-peace",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Python': '.py', 'C++': '.cpp', 'Java': '.java'}\n"
     ]
    }
   ],
   "source": [
    "object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "signed-restriction",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4431 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list assignment index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-205043ab6c19>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\".DS_Store\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m             \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdirec\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'L'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m255\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m             \u001b[0mlabels_init_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0my_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels_init_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"y_val.pkl\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"wb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list assignment index out of range"
     ]
    }
   ],
   "source": [
    "labels_init_val = defaultdict(list)\n",
    "# initialize label dictionary\n",
    "train_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/images/rgb\"\n",
    "for f in [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]:\n",
    "    labels_init[f[0:-4]] = [0,0,0,0,0,0]\n",
    "labels_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/val/labels/\"\n",
    "for ind, l in enumerate([f for f in os.listdir(labels_dir)][1:]):\n",
    "    direc = labels_dir + l\n",
    "    for file in tqdm([f for f in os.listdir(direc) if os.path.isfile(os.path.join(direc, f))]):\n",
    "        if file != \".DS_Store\":\n",
    "            rate = (np.array(Image.open(os.path.join(direc, file)).convert('L'))/255).mean()\n",
    "            labels_init_val[file[0:-4]][ind] = rate\n",
    "y_val = labels_init_val\n",
    "f = open(\"y_val.pkl\",\"wb\")\n",
    "pickle.dump(y_val,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleased-wyoming",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_init = defaultdict(list)\n",
    "# initialize label dictionary\n",
    "train_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/images/rgb\"\n",
    "for f in [f for f in os.listdir(train_dir) if os.path.isfile(os.path.join(train_dir, f))]:\n",
    "    labels_init[f[0:-4]] = [0,0,0,0,0,0]\n",
    "labels_dir = \"/Users/zhuqing/Documents/Github/Vision-for-Agriculture/Agriculture-Vision/train/labels/\"\n",
    "for ind, l in enumerate([f for f in os.listdir(labels_dir)][1:]):\n",
    "    direc = labels_dir + l\n",
    "    for file in tqdm([f for f in os.listdir(direc) if os.path.isfile(os.path.join(direc, f))]):\n",
    "        if file != \".DS_Store\":\n",
    "            rate = (np.array(Image.open(os.path.join(direc, file)).convert('L'))/255).mean()\n",
    "            labels_init[file[0:-4]][ind] = rate\n",
    "y_train = labels_init\n",
    "f = open(\"y_train.pkl\",\"wb\")\n",
    "pickle.dump(y_train,f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-lyric",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "fixed-population",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "y_train = pd.read_pickle('y_train.pkl')\n",
    "df = pd.DataFrame(y_train)\n",
    "df.to_csv('y_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faced-child",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = df.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "inappropriate-purchase",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 12901)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bearing-biodiversity",
   "metadata": {},
   "source": [
    "### DNN part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from imutils import paths\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "from glob import glob\n",
    "import IPython.display as display\n",
    "from IPython.display import clear_output\n",
    "import math\n",
    "import time\n",
    "from tensorflow.keras.layers import *\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "secure-spine",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
