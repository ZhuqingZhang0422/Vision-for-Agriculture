{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "single-trinity",
   "metadata": {},
   "source": [
    "# Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-serum",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X = torch.rand(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forbidden-finish",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "curious-antique",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch by default the data is float64 type, but we can always define the data type\n",
    "X = torch.rand([2,3], dtype = float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "associate-taylor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch operaton\n",
    "# add same with subscript\n",
    "X = torch.rand([2,3])\n",
    "Y = torch.rand([2,3])\n",
    "X + Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tough-drilling",
   "metadata": {},
   "outputs": [],
   "source": [
    "# when it comes to multiply it is usually element level\n",
    "X * Y "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprised-accent",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.rand([3,4])\n",
    "print(X)\n",
    "# print one column in tensor\n",
    "print(X[:,0])\n",
    "# print one element in tensor, use <.item()> function to select specific data element\n",
    "print(X[1,1].item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "relative-christian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape a tensor\n",
    "Y = X.view(-1,3)\n",
    "print(Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prompt-fountain",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "restricted-appointment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# When it comes to from torch to numpy, they share the same memory, then change torch will change the numpy array as well\n",
    "a = Y.numpy()\n",
    "Z = torch.from_numpy(a)\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "working-springfield",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device('cuda')\n",
    "    X = torch.ones(5, device = device)\n",
    "    y = torch.ones(5)\n",
    "    # switch the data into GPU\n",
    "    y.to(device)\n",
    "    Z = x + y\n",
    "    z = z.to('cpu')\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-locking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to specify requires_grad = True for gradiant calculation\n",
    "x = torch.ones(5, requires_grad = True)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-garbage",
   "metadata": {},
   "source": [
    "# Gradiant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-result",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "X = torch.randn(3, requires_grad = True)\n",
    "print(X)\n",
    "Y = X + 2\n",
    "print(Y)\n",
    "z = Y*Y*2\n",
    "z = z.mean()\n",
    "Z.backward(retain_graph=True)\n",
    "print(x.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-genesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if we don't need to track the tensor gradient there are three way to do that\n",
    "# 1. <x.requires_grad_(False)>\n",
    "# 2. <x.detach()>\n",
    "# 3. <with torch.no_grad():>\n",
    "X = torch.randn(3, requires_grad = True)\n",
    "print(X)\n",
    "X.requires_grad_(False)\n",
    "print(X)\n",
    "y = X + 2\n",
    "print(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rising-alpha",
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples\n",
    "import torch\n",
    "w = torch.ones(4,requires_grad = True)\n",
    "for epoch in range(3):\n",
    "    model_out = (w*3).sum()\n",
    "    model_out.backward()\n",
    "    print(w.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "touched-samoa",
   "metadata": {},
   "source": [
    "## Important detail\n",
    "rezero the gradiant for epoch to avoid accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-intermediate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is not correct since for each iteration the \n",
    "# gradiant is actually accumulate, should rezero the gradiant to start another epoch\n",
    "import torch\n",
    "w = torch.ones(4,requires_grad = True)\n",
    "for epoch in range(3):\n",
    "    model_out = (w*3).sum()\n",
    "    model_out.backward()\n",
    "    print(w.grad)\n",
    "    w.grad.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "found-joint",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example y = w*x \n",
    "x = torch.tensor(1.0)\n",
    "y = torch.tensor(2.0)\n",
    "w = torch.tensor(1.0, requires_grad = True)\n",
    "y_hat = w * x\n",
    "loss = (y_hat - y)**2\n",
    "\n",
    "# print the forward pass\n",
    "print(loss)\n",
    "\n",
    "# print the back propagation\n",
    "loss.backward()\n",
    "print(w.grad)\n",
    "\n",
    "#update the weight and iterate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "detailed-hierarchy",
   "metadata": {},
   "source": [
    "# Using pytorch API to specify the forward propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "golden-italy",
   "metadata": {},
   "source": [
    "## Np way of doing linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-respect",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Eg: f = w * x\n",
    "# f = 2 * x\n",
    "X = np.array([1,2,3,4], dtype = np.float32)\n",
    "y = np.array([2,4,6,8], dtype = np.float32)\n",
    "# initialize w \n",
    "w = 0.0\n",
    "\n",
    "# model prediction\n",
    "def forward(x):\n",
    "    return w * x\n",
    "# loss mean square error \n",
    "\n",
    "def loss(y, y_pred):\n",
    "    return ((y_pred - y)**2).mean()\n",
    "\n",
    "# graident \n",
    "# MSE = 1/N (w*X - y) **@\n",
    "# dj/dw = 1/N 2x (w*x - y)\n",
    "\n",
    "def gradient(x,y, y_pred):\n",
    "    return np.dot(2*x, y_pred - y).mean()\n",
    "\n",
    "print(f'Predic before training: f(5) = {forward(5):.3f}')\n",
    "             \n",
    "\n",
    "    \n",
    "# Training \n",
    "lr = 0.01\n",
    "n_iters = 40\n",
    "for epoch in range(n_iters):\n",
    "    # \n",
    "    y_pred = forward(X)\n",
    "    l = loss(y,y_pred)\n",
    "    dw = gradient(x,y,y_pred)\n",
    "    w -= lr * dw\n",
    "    if epoch %5 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {w:.3f}, loss = {l:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atlantic-agriculture",
   "metadata": {},
   "source": [
    "## Using pytorch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enclosed-november",
   "metadata": {},
   "source": [
    "### Stage one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "understood-feature",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predic before training: f(5) = 0.000\n",
      "epoch 1: w = 0.300, loss = 30.000\n",
      "epoch 3: w = 0.772, loss = 15.660\n",
      "epoch 5: w = 1.113, loss = 8.175\n",
      "epoch 7: w = 1.359, loss = 4.267\n",
      "epoch 9: w = 1.537, loss = 2.228\n",
      "epoch 11: w = 1.665, loss = 1.163\n",
      "epoch 13: w = 1.758, loss = 0.607\n",
      "epoch 15: w = 1.825, loss = 0.317\n",
      "epoch 17: w = 1.874, loss = 0.165\n",
      "epoch 19: w = 1.909, loss = 0.086\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "W = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "def forward(X):\n",
    "    return W * X\n",
    "\n",
    "def loss(Y,Y_pred):\n",
    "    return ((Y_pred - Y)**2).mean()\n",
    "\n",
    "print(f'Predic before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "lr = 0.01 \n",
    "n_iters = 20\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    l.backward() # calculate dl/dw\n",
    "    # update weights\n",
    "    '''\n",
    "    # this is very important\n",
    "    since we don;t want W to show up in the computational graph we have to run in no grad mode\n",
    "    '''\n",
    "    with torch.no_grad():\n",
    "        W -= lr * W.grad\n",
    "    \n",
    "    #zero gradients\n",
    "    '''\n",
    "    # this is very important\n",
    "    avoid the accumulation of gradiant\n",
    "    '''\n",
    "    W.grad.zero_()\n",
    "    \n",
    "    if epoch %2 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {W:.3f}, loss = {l:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bizarre-aging",
   "metadata": {},
   "source": [
    "### Stage two\n",
    "    use build in functions to build the structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "adjusted-future",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1: w = 0.300, loss = 30.000\n",
      "epoch 11: w = 1.665, loss = 1.163\n",
      "epoch 21: w = 1.934, loss = 0.045\n",
      "epoch 31: w = 1.987, loss = 0.002\n",
      "epoch 41: w = 1.997, loss = 0.000\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Workflow \n",
    "# 1) Design model (input, output sizem foward pass)\n",
    "# 2) Construct loss and optimizer\n",
    "# 3) Train loop\n",
    "     - forward pass: compute prediction\n",
    "     - backward pass: gradients\n",
    "     - update weights\n",
    "'''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "X = torch.tensor([1,2,3,4], dtype = torch.float32)\n",
    "Y = torch.tensor([2,4,6,8], dtype = torch.float32)\n",
    "W = torch.tensor(0.0, dtype = torch.float32, requires_grad = True)\n",
    "\n",
    "def forward(X):\n",
    "    return W * X\n",
    "\n",
    "lr = 0.01 \n",
    "n_iters = 50\n",
    "\n",
    "loss = nn.MSELoss()\n",
    "optimizer = torch.optim.SGD([W], lr = lr)\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    # prediction = forward pass\n",
    "    y_pred = forward(X)\n",
    "    \n",
    "    # loss\n",
    "    l = loss(Y,y_pred)\n",
    "    \n",
    "    # gradiant = back propagation, update the computational graph \n",
    "    l.backward()\n",
    "    \n",
    "    # update weights\n",
    "    optimizer.step()\n",
    "    \n",
    "    #zero gradiants\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    if epoch %10 == 0:\n",
    "        print(f'epoch {epoch + 1}: w = {W:.3f}, loss = {l:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-batman",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noted-denmark",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "looking-generation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "involved-bread",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-enclosure",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "empty-boost",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-wealth",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recovered-silly",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "arranged-laser",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moving-scratch",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "liberal-timeline",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "macro-breathing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
